{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Restricted Boltzman Machine \n",
    "\n",
    "A restricted Boltzmann Machine is an \"Energy Based\" generative stochastic model. They were initially invented by Paul Smolensky in 1986 and were called \"Harmonium\". After the evolution of training algorithms in the mid 2000's by Geoffrey Hinton, the boltzman machine became more prominent. They gained big popularity in recent years in the context of the Netflix Prize where RBMs achieved state of the art performance in collaborative filtering and have beaten most of the competition.\n",
    "\n",
    "RBM's are useful for dimensionality reduction, classification, regression, collaborative filtering, feature learning and topic modeling. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Architecture\n",
    "RBMs are shallow, two-layer neural nets that constitute the building blocks of deep-belief networks. The first layer of the RBM is called the visible, or input, layer, and the second is the hidden layer. The absense of an output layer is apparent. As we move forward we would learn why the output layer won't be needed.\n",
    "<img src=\"figure1.png\" width=\"150\" height=\"50\" title=\"Layers in RBM\">\n",
    "Figure1: Layers in RBM\n",
    "Each circle in the figure above represents a neuron-like unit called a node, and nodes are simply where calculations take place. \n",
    "<img src=\"figure3.png\" width=\"500\" height=\"200\" title=\"Layers in RBM\">\n",
    "Figure2: Structure of RBM\n",
    "The nodes are connected to each other across layers, but no two nodes of the same layer are linked. That is, there is no intra-layer communication – this is the restriction in a restricted Boltzmann machine. Each node is a locus of computation that processes input, and begins by making stochastic decisions about whether to transmit that input or not. Each visible node takes a low-level feature from an item in the dataset to be learned."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start by importing the packages that will be required in this project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from collections import Counter, defaultdict\n",
    "import pandas as pd\n",
    "from scipy.sparse import coo_matrix, hstack\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "np_rng = np.random.RandomState(1234) #setting the random state\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# graded\n",
    "# import data\n",
    "\n",
    "df = pd.read_excel(\"~/RBM Assignment/amazon.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>So there is no way for me to plug it in here i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Good case, Excellent value.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Great for the jawbone.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Tied to charger for conversations lasting more...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>The mic is great.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Text\n",
       "0  So there is no way for me to plug it in here i...\n",
       "1                        Good case, Excellent value.\n",
       "2                             Great for the jawbone.\n",
       "3  Tied to charger for conversations lasting more...\n",
       "4                                  The mic is great."
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#run this and check if you have got the correct output\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In Topic Modelling, you find the best set of topics that describe the document. There are various ways to perform topic modelling one of which is RBM. You train your RBM on a set of documents. \n",
    "The visible layers will be the words in the text, the hidden layers will give the Topics. \n",
    "To input words into the visible layer, let's convert the train and test data you split above into a bag of words model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#graded\n",
    "# create bag of words model on train and test data\n",
    "\n",
    "tf = CountVectorizer()\n",
    "# tf = CountVectorizer(stop_words='english')\n",
    "#the final shape should be (number of documents, vocabulary)\n",
    "\n",
    "# fit tf on the dataframe df\n",
    "tf = tf.fit(df['Text'])\n",
    "\n",
    "# transform df dataframe\n",
    "trainX = tf.transform(df['Text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(1000, 1847)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#check if you are getting the correct output\n",
    "print(sum(trainX.toarray()[1]))\n",
    "trainX.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that you have the bag of words model, let's define the number of visible and hidden units."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#graded\n",
    "# define visible units\n",
    "visibleUnits = trainX.shape[1]       # vocabulary size ~1 line\n",
    "\n",
    "# assign number of units\n",
    "hiddenUnits = 5 # hyperparameter, this means that we are looking for 5 topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1847"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "visibleUnits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#graded\n",
    "# utility Functions\n",
    "\n",
    "# deine the sigmoid function\n",
    "def sigmoid(X):\n",
    "    return 1/(1 + np.exp(-X))     # ~ 1 line"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RBM as a Probabilistic Model\n",
    "Restricted Boltzmann Machines are probabilistic. As opposed to assigning discrete values the model assigns probabilities. At each point in time the RBM is in a certain state. The state refers to the values of neurons in the visible and hidden layers v and h. The probability that a certain state of v and h can be observed is given by the following joint distribution:\n",
    "<img src=\"figure4.png\" width=\"200\" height=\"70\" title=\"Layers in RBM\">\n",
    "\n",
    "Eq. 2. Joint Distribution for v and h.\n",
    "Here Z is called the ‘partition function’ that is the summation over all possible pairs of visible and hidden vectors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The joint distribution is known as the Boltzmann Distribution which gives the probability that a particle can be observed in the state with the energy E. Unfortunately it is very difficult to calculate the joint probability due to the huge number of possible combination of v and h in the partition function Z. Much easier is the calculation of the conditional probabilities of state h given the state v and conditional probabilities of state v given the state h:\n",
    "<img src=\"figure5.png\" width=\"200\" height=\"70\" title=\"Layers in RBM\">\n",
    "\n",
    "Eq. 3. Conditional probabilities for h and v.\n",
    "It should be noticed beforehand (before demonstrating this fact on practical example) that each neuron in a RBM can only exist in a binary state of 0 or 1. The most interesting factor is the probability that a hidden or visible layer neuron is in the state 1 — hence activated. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Contrastive Divergence\n",
    "\n",
    "### Gibbs Sampling\n",
    "The first part of the training is called Gibbs Sampling. Given an input vector v we are using p(h|v) for prediction of the hidden values h via sampling. Knowing the hidden values we use p(v|h) for prediction of new input values v via sampling. This process is repeated k times. After k iterations, we obtain the visible vector $v_k$ which was recreated from original input values $v_0$.\n",
    "<img src=\"figure8.png\" width=\"500\" height=\"300\" title=\"Layers in RBM\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The gibbs function *gibbs* is divided into subparts: <br>\n",
    "1.*sampleHiddenLayer * <br>\n",
    "2.*sampleVisibleLayer*\n",
    "\n",
    "Let's look at *sampleHiddenLayer* now.\n",
    "\n",
    "### Sample Hidden Layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You already know that given an input vector v the probability for a single hidden neuron j being activated is:\n",
    "<img src=\"figure6.png\" width=\"400\" height=\"200\" title=\"Layers in RBM\">\n",
    "\n",
    "Eq. 4\n",
    "Here is σ the Sigmoid function.\n",
    "\n",
    "*sampleHiddenLayer* takes the visible layer as input to calculate the hidden layer using Eq. 4 *h1Pdf* and then samples it to get * h1_sample*\n",
    "\n",
    "    v_sample: given visible layer matrix; matrix because a batch of data points will be trained at one go\n",
    "    returns a sample vector of hidden layer and its distribution for a batch of data points\n",
    "    \n",
    "    hPdf: distribution of hidden layer; a matrix for batch of datapoints = p(h|v)\n",
    "    h_sample: sampled hidden layer matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#graded\n",
    "def sampleHiddenLayer(v_sample):\n",
    "    \n",
    "#     print(\"v_sample ==>\", v_sample.shape)\n",
    "#     print(\"Transpose of W ==>\", np.transpose(W).shape)\n",
    "#     print(\"W ==>\", W.shape)\n",
    "    \n",
    "    # write the code for calculation of hPdf using vectorized implementation of Eq 4\n",
    "    hPdf = sigmoid(hiddenBias + np.dot(v_sample, W))\n",
    "    \n",
    "    # Here, np.random.binomial is used to create the hidden layer sample matrix\n",
    "    h_sample = np_rng.binomial(size=hPdf.shape, n=1, p=hPdf)\n",
    "    return [hPdf, h_sample]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sample Visible Layer\n",
    "Similarly, the probability that a binary state of a visible neuron i is set to 1 is:\n",
    "<img src=\"figure7.png\" width=\"400\" height=\"200\" title=\"Layers in RBM\">\n",
    "Eq. 5\n",
    "\n",
    "As seen in equation 5, we will be writing a function to sample the Visible Layer.\n",
    "This function samples the visible layer based on the sampled data of hidden layer. <br>\n",
    "\n",
    "There are some differences in writing the function *sampleVisibleLayer*. <br>Firstly, we use np.random.multinomial to sample the visible layer *v_sample* from the distribution *vPdf*. <br>Secondly,elements of *vPdf* needs to sum to 1 as the function np.random.multinomial used to sample the visible layer takes on probability distributions as *pvals*. In other words, you are finding the softmax values. <br> Thirdly, we also make use of the *D* to sample the visible layer as each document has different word count.\n",
    "    \n",
    "    h_sample: given hidden layer matrix; matrix because a batch of data points will be trained at one go\n",
    "    D: array of the sum of the row of the data vector; vector containing number of words in each document\n",
    "    \n",
    "    returns a sample vector of hidden layer and its distribution for a batch of data points\n",
    "    \n",
    "    vPdf: distribution of visible layer; a matrix for batch of datapoints = p(v|h)\n",
    "    v_sample: sampled visible layer matrix\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#graded\n",
    "def sampleVisibleLayer(h_sample, D):\n",
    "    \n",
    "    # complete the following function such that vPdf has the sum of entries equal to 1 for each of the datapoints in the batch\n",
    "    # you have to use axis = 1 in writing the denominator\n",
    "    # numerator = sigmoid(visibleBias + np.dot(W, h_sample))    # ~1 line   ==> Original\n",
    "    \n",
    "#     print(\"h_sample ==>\", h_sample.shape)\n",
    "#     print(\"Transpose of W ==>\", np.transpose(W).shape)\n",
    "    # print(\"Transpose of h_sample ==>\", np.transpose(h_sample.shape))\n",
    "    \n",
    "    numerator = sigmoid(visibleBias + np.dot(h_sample, np.transpose(W)))\n",
    "    \n",
    "    denominator = numerator.sum(axis=1).reshape(numerator.shape[0], 1)      # ~1 line\n",
    "    vPdf = numerator/denominator   # ~1 line\n",
    "    \n",
    "    # Here np.random.multinomial is used to sample as each document has different number of words \n",
    "    # and hence D is also a parameter in sampling\n",
    "    v_sample = np.zeros((batchSize, vPdf.shape[1]))\n",
    "    \n",
    "    for i in range(batchSize):\n",
    "        v_sample[i] = np_rng.multinomial(size=1, n=D[i], pvals=vPdf[i])\n",
    "    \n",
    "    return [vPdf, v_sample]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sampling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use the above functions to write the function *gibbs* to run one iteration of gibbs sampling. Note that we are calculating the visible layer samples first and then using it to calculate he hidden layer sample. It'll become clear soon why we are doing so when you write the function for Contrastive Divergence.\n",
    "    \n",
    "    Input:\n",
    "    h_sample: given hidden layer matrix; matrix because a batch of data points will be trained at one go\n",
    "    D: array of the sum of the row of the data vector; vector containing number of words in each document\n",
    "    \n",
    "    Output:\n",
    "    vPdf: distribution of visible layer\n",
    "    v_sample: sampled visible layer matrix\n",
    "    hPdf: distribution of hidden layer\n",
    "    h_sample: sampled hidden layer matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#graded\n",
    "def gibbs(h_sample, D):\n",
    "    \n",
    "    #use sampleVIsibleLayer and sampleHiddenLayer \n",
    "    vPdf, v_sample = sampleVisibleLayer(h_sample, D)[0], sampleVisibleLayer(h_sample, D)[1]      # ~1 line\n",
    "    hPdf, h_sample = sampleHiddenLayer(v_sample)    # ~1 line\n",
    "    return [vPdf, v_sample, hPdf, h_sample]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Contrastive Divergence\n",
    "\n",
    "You have learned that Contrastive Divergence updates weights after one iteration of Gibbs Sampling. Here, we shall perform *k* such iterations in Contrastive Divergence. \n",
    "The update of the weight matrix happens post the Contrastive Divergence step. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Now, we will writing a funtion to run the contrastive divergence algorithm in k steps\n",
    "    \n",
    "    Input:\n",
    "    data: batch data (visible layer)\n",
    "    k: no of iterations for gibbs sampling\n",
    "    \n",
    "    Output:\n",
    "    hiddenPDF_data: distribution of the hidden layer based on data\n",
    "    visibleSamples: visible samples generated by gibbs sampling \n",
    "    hiddenPDF: distribution of the hidden layer based on samples generated by gibbs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#graded\n",
    "def cd_k(data,k):\n",
    "    \n",
    "    D = data.sum(axis=1) \n",
    "    hiddenPDF_data, hiddenSample_data = sampleHiddenLayer(data)     # sample the hidden layer using the input data\n",
    "    chain_start = hiddenSample_data\n",
    "\n",
    "    for step in range(k):\n",
    "        if step == 0:\n",
    "            visiblePDF, visibleSamples, hiddenPDF, hiddenSamples = gibbs(chain_start, D)  # perform gibbs sampling using chain_start\n",
    "        else:\n",
    "            visiblePDF, visibleSamples, hiddenPDF, hiddenSamples = gibbs(hiddenSamples, D)  # perform gibbs sampling using hiddenSamples\n",
    "    return hiddenPDF_data, visibleSamples, hiddenPDF\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the Model\n",
    "\n",
    "Vectors $v_0$ and $v_k$ are used to calculate the activation probabilities for hidden layers $p(h_0|v_0)$ and $p(h_k|v_k)$ (Eq.4). The difference between the outer products of those probabilities with input vectors $v_0$ and $v_k$ results in the update matrix:\n",
    "<img src=\"figure9.png\" width=\"300\" height=\"200\" title=\"Layers in RBM\">\n",
    "Eq. 6. Update matrix. **Figure out** the vectorized implementation for this.\n",
    "\n",
    "In order to calculate $\\Delta (bias)$, <br>\n",
    "<center>$\\Delta (visiblebias) = average\\_ across\\_ batch(v_0 - v_k)$ </center> \n",
    "<center>$\\Delta (hiddenbias) = average\\_across\\_ batch(p(h_0|v_0) - p(h_k|v_k))$ </center> \n",
    "\n",
    "Using the update matrix the new weights can be calculated with momentum gradient ascent, given by:\n",
    "<center>  $mW_t = \\gamma \\ mW_{t-1} - \\Delta W$</center> \n",
    "<center>  $mvisiblebias_t = \\gamma \\ mvisiblebias_{t-1} - \\Delta visiblebias$</center>\n",
    "<center>  $mhiddenbias_t = \\gamma \\ mhiddenbias_{t-1} - \\Delta hiddenbias$</center><br>\n",
    "<center>  $W_t = W_{t-1} + \\alpha \\ mW_t$</center> \n",
    "<center>  $visiblebias_t = visiblebias_{t-1} + \\alpha \\ mvisiblebias_t$</center> \n",
    "<center>  $hiddenbias_t = hiddenbias_{t-1} + \\alpha \\ mhiddenbias_t$</center> \n",
    "\n",
    "\n",
    "Eq. 7. Update rule for the weights.\n",
    "\n",
    "Note that in the code implementation below <br>\n",
    " hiddenPDF_data is $p(h_0|v_0)$ <br>\n",
    " visibleSamples is $v_k$ <br>\n",
    " hiddenPDF is $p(h_k|v_k)$ <br>\n",
    " mdata is $v_0$ <br>\n",
    " eta is $\\alpha$ <br>\n",
    " mrate is $\\gamma$ <br>\n",
    "These will be helpful in writing the weight updates.\n",
    "\n",
    "In this we will write a function which iterates over our data for *epochs*.\n",
    "At every epoch we shuffle the data and then run CD on a mini batch size defined by *batchSize*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initializations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "visibleUnits: no of words in your Bag of words Model\n",
    "hiddenUnits: no of topics\n",
    "batchSize: data slice to be selected \n",
    "epochs: no of iterations\n",
    "eta: learning rate\n",
    "mrate: momentum coefficient\n",
    "W : weights between the visible and hidden layer\n",
    "visibleBias, hiddenBias: biases for visible and hidden layer respectively\n",
    "\"\"\"\n",
    "\n",
    "# define batch size\n",
    "batchSize = 200\n",
    "\n",
    "epochs = 100\n",
    "eta = 0.05 \n",
    "mrate = 0.3\n",
    "\n",
    "# initialise weights\n",
    "weightinit=0.01\n",
    "W = weightinit * np_rng.randn(visibleUnits, hiddenUnits)\n",
    "visibleBias = weightinit * np_rng.randn(visibleUnits)\n",
    "hiddenBias = np.zeros((hiddenUnits))\n",
    "\n",
    "# we shall use 5 interations of gibbs sampling\n",
    "k=5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#graded\n",
    "def train(dataX,k):\n",
    "    \n",
    "    ## We can initialize these momentum parameters to zero ie; mW , mvisibleBias & mhiddenBias which can be updated during training.\n",
    "    mW = np.zeros((visibleUnits, hiddenUnits))    # initialise momentum_weights\n",
    "    mvisibleBias =  np.zeros(visibleUnits)        # initialise momentum_visiblebiases\n",
    "    mhiddenBias =  np.zeros(hiddenUnits)          # initialise momentum_hiddenbiases\n",
    "    global W,visibleBias,hiddenBias,mrate,batchSize,epochs # calling the variables initiliazed at the begining\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        np_rng.shuffle(dataX) #shuffling the data\n",
    "        \n",
    "        for i in range(0, dataX.shape[0], batchSize):\n",
    "            mData = dataX[i:i+batchSize]      #select a batch of datapoints\n",
    "            hiddenPDF_data, visibleSamples, hiddenPDF =  cd_k(mData,k) # Perform Contrastive Divergence on the batch for k iterations  \n",
    "            \n",
    "            mW = mrate*mW - (np.dot(mData.T, hiddenPDF_data) - np.dot(visibleSamples.T, hiddenPDF))\n",
    "            \n",
    "            #write the momentum update equation for weight matrix\n",
    "            mvisibleBias = mrate*mvisibleBias - np.mean(mData - visibleSamples)\n",
    "            \n",
    "            #write the momentum update equation for visiblebias vector\n",
    "            mhiddenBias = mrate*mhiddenBias - np.mean(hiddenPDF_data - hiddenPDF)\n",
    "            \n",
    "            #write the momentum update equation for hiddenbias vector\n",
    "            \n",
    "            W = W + eta*mW                                  #weight update equation\n",
    "            visibleBias = visibleBias + eta*mvisibleBias    #visible bias update equation\n",
    "            hiddenBias = hiddenBias + eta*mhiddenBias       #hidden bias update equation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the Model\n",
    "\n",
    "This will take around 10 minutes of time to run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "train(trainX.toarray(),k)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word Distribution based on Topics\n",
    "In this function, we are finding the distribution of words over the topics. You can take a look at the words under each topic and see what they are talking about. The number of topics is the number of neurons in the hidden layer. <br>\n",
    "<br>\n",
    "\n",
    "For each topic, the function prints the top 15 words that describe the topic. You can see that some of the words occur in multiple topics.\n",
    "\n",
    "    topic: number of hidden layers\n",
    "    voc: indexing of the vocabulary\n",
    "    \n",
    "Feel free to change the number of iterations of gibbs sampling in Contrastive Divergence and see how the distribution of words change under the topics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def worddist( topic, voc):\n",
    "    \n",
    "    \"\"\"\n",
    "    Initialize every topic =1 once \n",
    "    \"\"\"\n",
    "    vecTopics = np.zeros((topic, topic))\n",
    "    for i in range(len(vecTopics)):\n",
    "        vecTopics[i][i] = 1\n",
    "    \n",
    "    for i, vecTopic in enumerate(vecTopics):\n",
    "       \n",
    "        numerator = np.exp(np.dot(vecTopic, W.T) + visibleBias)\n",
    "        denominator = numerator.sum().reshape((1, 1))\n",
    "        word_distribution = (numerator/denominator).flatten()\n",
    "        \n",
    "        tmpDict = {}\n",
    "        for j in voc.keys():\n",
    "            tmpDict[j] = word_distribution[voc[j]]\n",
    "        print('topic', str(i), ':', vecTopic)\n",
    "        lm=0\n",
    "        for word, prob in sorted(tmpDict.items(), key=lambda x:x[1], reverse=True):\n",
    "            print ( word, str(prob))\n",
    "            lm+=1\n",
    "            if lm==15:\n",
    "                break\n",
    "        print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "topic 0 : [1. 0. 0. 0. 0.]\n",
      "behing 0.041658531257466755\n",
      "getting 0.029663916342009367\n",
      "pack 0.02577990381829511\n",
      "bougth 0.0254876386312819\n",
      "keypads 0.015511131980801013\n",
      "procedure 0.013212926281474306\n",
      "compared 0.012262062426055645\n",
      "improve 0.011854703604777422\n",
      "replaced 0.011709482027664173\n",
      "especially 0.011570035317640448\n",
      "fourth 0.010404031875489833\n",
      "defeats 0.01016488235802411\n",
      "shield 0.010131844273259735\n",
      "thin 0.010108969598241022\n",
      "disgusting 0.009525122115666838\n",
      "\n",
      "\n",
      "topic 1 : [0. 1. 0. 0. 0.]\n",
      "behing 0.04040541387334558\n",
      "getting 0.028949952982628754\n",
      "pack 0.025246487276862416\n",
      "bougth 0.025199856573490737\n",
      "keypads 0.015337460639244682\n",
      "procedure 0.013292474502089322\n",
      "compared 0.012417757715640701\n",
      "improve 0.012260818069361263\n",
      "replaced 0.011723629690866853\n",
      "especially 0.011330512173095929\n",
      "fourth 0.010493443740321467\n",
      "thin 0.010308399905767233\n",
      "defeats 0.01020873802822609\n",
      "shield 0.010163372862264953\n",
      "buyit 0.009398821676161595\n",
      "\n",
      "\n",
      "topic 2 : [0. 0. 1. 0. 0.]\n",
      "behing 0.04031814367673754\n",
      "getting 0.029289643520307405\n",
      "pack 0.0257247178297293\n",
      "bougth 0.025167644074906563\n",
      "keypads 0.015354535605245898\n",
      "procedure 0.013553126063907624\n",
      "compared 0.012203943230715818\n",
      "improve 0.012000370679688909\n",
      "replaced 0.011904300517677144\n",
      "especially 0.011368942734474972\n",
      "defeats 0.010367754222253667\n",
      "fourth 0.010285000210659265\n",
      "shield 0.010152669415777173\n",
      "thin 0.010096639159458683\n",
      "disgusting 0.009638312935330767\n",
      "\n",
      "\n",
      "topic 3 : [0. 0. 0. 1. 0.]\n",
      "behing 0.040882757949120795\n",
      "getting 0.029073658633760713\n",
      "pack 0.025611915481794367\n",
      "bougth 0.02559237704923862\n",
      "keypads 0.015407383477176509\n",
      "procedure 0.013231608810564224\n",
      "compared 0.012221222054099772\n",
      "replaced 0.011940394976210385\n",
      "improve 0.011853647622477224\n",
      "especially 0.01150067343003729\n",
      "defeats 0.010510292438435806\n",
      "thin 0.010337461354516974\n",
      "fourth 0.010290532983906671\n",
      "shield 0.01024971591764493\n",
      "disgusting 0.009537165982458205\n",
      "\n",
      "\n",
      "topic 4 : [0. 0. 0. 0. 1.]\n",
      "behing 0.041070743375784334\n",
      "getting 0.029252702906622432\n",
      "pack 0.025661474045504076\n",
      "bougth 0.024999575479630276\n",
      "keypads 0.01565226333775277\n",
      "procedure 0.01337409283994576\n",
      "compared 0.012477586536184906\n",
      "improve 0.012071359825812658\n",
      "replaced 0.011851915316385521\n",
      "especially 0.01147846924817369\n",
      "defeats 0.010428501944801985\n",
      "fourth 0.010282141374419422\n",
      "thin 0.010200131572158754\n",
      "shield 0.010152387479328452\n",
      "disgusting 0.009580605669428158\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "worddist( hiddenUnits, tf.vocabulary_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output shows the probability assigned to each word for ever topic present."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
